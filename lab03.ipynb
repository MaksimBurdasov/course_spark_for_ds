{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929b3051-1037-4cfd-9bb9-7bcf6dca4e21",
   "metadata": {},
   "source": [
    "# Лаб3. Рекомендательная система видеоконтента с implicit feedback – Spark ML\n",
    "Задание: https://github.com/newprolab/sber-spark-ds-18/blob/main/labs/lab03.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc477d9d-b0ca-432c-974d-4c4b0e68b868",
   "metadata": {},
   "source": [
    "### Библиотеки и контекст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950a2e96-d763-4a8d-99af-71a8eab8c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark-3.4.3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b17a28-babf-4040-adbb-1f24642fc8b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://repos.spark-packages.org/ added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /data/home/maksim.burdasov/.ivy2/cache\n",
      "The jars for the packages stored in: /data/home/maksim.burdasov/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a4b78f56-9c15-4f0a-a7f9-d003e3bc4fe8;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.4.3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.3 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.3 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 489ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.3 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.3 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a4b78f56-9c15-4f0a-a7f9-d003e3bc4fe8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/9ms)\n",
      "25/04/22 10:37:24 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/04/22 10:37:29 WARN yarn.Client: Same path resource file:///data/home/maksim.burdasov/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.3.jar added multiple times to distributed cache.\n",
      "25/04/22 10:37:29 WARN yarn.Client: Same path resource file:///data/home/maksim.burdasov/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.3.jar added multiple times to distributed cache.\n",
      "25/04/22 10:37:29 WARN yarn.Client: Same path resource file:///data/home/maksim.burdasov/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar added multiple times to distributed cache.\n",
      "25/04/22 10:37:29 WARN yarn.Client: Same path resource file:///data/home/maksim.burdasov/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar added multiple times to distributed cache.\n",
      "25/04/22 10:37:29 WARN yarn.Client: Same path resource file:///data/home/maksim.burdasov/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar added multiple times to distributed cache.\n",
      "25/04/22 10:37:29 WARN yarn.Client: Same path resource file:///data/home/maksim.burdasov/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar added multiple times to distributed cache.\n",
      "25/04/22 10:37:29 WARN yarn.Client: Same path resource file:///data/home/maksim.burdasov/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar added multiple times to distributed cache.\n",
      "25/04/22 10:37:29 WARN yarn.Client: Same path resource file:///data/home/maksim.burdasov/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.5.jar added multiple times to distributed cache.\n",
      "25/04/22 10:37:29 WARN yarn.Client: Same path resource file:///data/home/maksim.burdasov/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar added multiple times to distributed cache.\n",
      "25/04/22 10:37:29 WARN yarn.Client: Same path resource file:///data/home/maksim.burdasov/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar added multiple times to distributed cache.\n",
      "25/04/22 10:37:29 WARN yarn.Client: Same path resource file:///data/home/maksim.burdasov/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import re\n",
    "\n",
    "conf = SparkConf()\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf)\n",
    "    .appName('max_burdasov_lab3')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94234a29-5dc7-4403-8530-e2b1c777ec2e",
   "metadata": {},
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "82a99e6e-4377-471d-b3ad-2690ab87a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   3 hdfs hdfs   91066524 2022-01-06 18:46 /labs/slaba03/laba03_items.csv\n",
      "-rw-r--r--   3 hdfs hdfs   29965581 2022-01-06 18:46 /labs/slaba03/laba03_test.csv\n",
      "-rw-r--r--   3 hdfs hdfs   74949368 2022-01-06 18:46 /labs/slaba03/laba03_train.csv\n",
      "-rw-r--r--   3 hdfs hdfs  871302535 2022-01-06 18:46 /labs/slaba03/laba03_views_programmes.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a443de11-04dc-48f6-896f-30d2d11ac97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Таргеты\n",
    "train_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('csv')\n",
    "    .option('header', 'true')\n",
    "    .option('inferSchema', 'true')\n",
    "    .load('/labs/slaba03/laba03_train.csv')\n",
    ")\n",
    "\n",
    "train_df = train_df.cache()\n",
    "train_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9978a8a3-623d-4fca-84f8-db5d5d00fde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------------\n",
      " item_id                     | 65667                                         \n",
      " channel_id                  | null                                          \n",
      " datetime_availability_start | 1970-01-01 03:00:00                           \n",
      " datetime_availability_stop  | 2018-01-01 03:00:00                           \n",
      " datetime_show_start         | null                                          \n",
      " datetime_show_stop          | null                                          \n",
      " content_type                | 1                                             \n",
      " title                       | на пробах только девушки (all girl auditions) \n",
      " year                        | 2013.0                                        \n",
      " genres                      | Эротика                                       \n",
      " region_id                   | null                                          \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Данные о передачах\n",
    "items_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('csv')\n",
    "    .option(\"delimiter\", \"\\t\")\n",
    "    .option('header', 'true')\n",
    "    .option('inferSchema', 'true')\n",
    "    .load('/labs/slaba03/laba03_items.csv')\n",
    ")\n",
    "\n",
    "items_df = items_df.cache()\n",
    "items_df.show(1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba947e1-c9e9-4481-860b-468f321c3e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+----------+---------+\n",
      "|user_id|item_id|ts_start  |ts_end    |item_type|\n",
      "+-------+-------+----------+----------+---------+\n",
      "|0      |7101053|1491409931|1491411600|live     |\n",
      "|0      |7101054|1491412481|1491451571|live     |\n",
      "|0      |7101054|1491411640|1491412481|live     |\n",
      "|0      |6184414|1486191290|1486191640|live     |\n",
      "|257    |4436877|1490628499|1490630256|live     |\n",
      "|1654   |7489015|1493434801|1493435401|live     |\n",
      "|1654   |7489023|1493444101|1493445601|live     |\n",
      "|1654   |6617053|1489186156|1489200834|live     |\n",
      "|1654   |6438693|1487840070|1487840433|live     |\n",
      "|1654   |6526859|1488705452|1488706154|live     |\n",
      "|1654   |6526754|1488532396|1488532895|pvr      |\n",
      "|1654   |6239098|1486732011|1486732410|live     |\n",
      "|1654   |6438763|1488305761|1488307286|pvr      |\n",
      "|1654   |7489013|1493433301|1493434201|live     |\n",
      "|1654   |6317094|1486829784|1486830389|live     |\n",
      "+-------+-------+----------+----------+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Дополнительная информация\n",
    "views_programmes_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('csv')\n",
    "    .option('header', 'true')\n",
    "    .option('inferSchema', 'true')\n",
    "    .load('/labs/slaba03/laba03_views_programmes.csv')\n",
    ")\n",
    "\n",
    "views_programmes_df = views_programmes_df.cache()\n",
    "views_programmes_df.show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d387bb5-0b13-4a4e-9a18-c0d0329456d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  94814|    null|\n",
      "+-------+-------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Данные для инференса\n",
    "infer_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format('csv')\n",
    "    .option('header', 'true')\n",
    "    .option('inferSchema', 'true')\n",
    "    .load('/labs/slaba03/laba03_test.csv')\n",
    ")\n",
    "\n",
    "infer_df = infer_df.cache()\n",
    "infer_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f7be7-ca24-450a-b830-fddc5457848e",
   "metadata": {},
   "source": [
    "### Препроцессинг\n",
    "Обработка данных и подготовка фичей для финального датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a342a-bb61-4b38-85e5-144f40528141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: подготовить фичи из сырых данных\n",
    "* фичи о программах (айтемах):\n",
    "    + количество покупок этого айтема\n",
    "    + количество записей по этому айтему в датафрейме\n",
    "    + доля покупок этого айтема из всех записей по этому айтому\n",
    "    + доля покупок этого айтема из всех покупок\n",
    "    + лет с момента выпуска\n",
    "    - смотрит ли пользователь этот канал\n",
    "* фичи о пользователях\n",
    "    + общее количество покупок пользователя\n",
    "    + доля платных программ, которые пользовать приобрел\n",
    "    + отношение количества покупок пользователя к среднему количествую покупок\n",
    "    + перцентиль пользователя по количеству покупок\n",
    "* фичи о пристрастиях пользователей (разделить на записанные и просмотренные в лайве)\n",
    "    - общее количество сеансов\n",
    "    - общее время просмотра\n",
    "    - среднее время просмотра\n",
    "    - среднее время перерывов между просмотрами\n",
    "    - времени с первой просмотренной программы до текущего момента\n",
    "    - времени просмотра платных программ\n",
    "    - среднее время просмотра платных программ\n",
    "    - доля просмотра платных программ\n",
    "    - доля платных программ из всех уникальных отсмотренных программ\n",
    "    - (множество колонок) время и доля просмотра каждого жанра от общих времени и количества просмотров\n",
    "    - среднее количество лет с выпуска программ, которые он смотрит\n",
    "    - топ 1 самый просматриваемый пользователем канал\n",
    "    - топ 2 самый просматриваемый пользователем канал\n",
    "    - топ 3 самый просматриваемый пользователем канал\n",
    "    - регион, программы из которых больше всего смотрел\n",
    "* кластера для программ\n",
    "    + методом плеча определить оптимальное количество кластеров\n",
    "    + выполнить кластеризацию по описаниям\n",
    "    - (если останется время) добавить колонки с вероятностями принадлежности к кластерам\n",
    "* (если останется время) кластеризация пользователей\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44fe1455-470a-451f-83ea-8b5986230535",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Исследование данных\n",
    "\n",
    "# # сколько всего пользователей (1941)\n",
    "# train_df.select('user_id').distinct().count()\n",
    "\n",
    "# # проверить, что у каждого пользователя одинакого строк (разное, но их от ~2400 до 2690)\n",
    "# train_df.groupBy('user_id').count().agg({'count': 'max'}).show()\n",
    "\n",
    "# # есть ли дубли в train_df (нет)\n",
    "# print(train_df.count() - train_df.select('user_id', 'item_id').distinct().count())\n",
    "\n",
    "# # сколько вообще покупок в таблице (10904 out of 5032624)\n",
    "# ttl_cnt = train_df.count()\n",
    "# prch_cnt = train_df.filter(F.col('purchase') == 1).count()\n",
    "# print(ttl_cnt, prch_cnt)\n",
    "\n",
    "# # уникальных программ (635'568)\n",
    "# items_df.select('item_id').count()\n",
    "\n",
    "# # всего платных программ (3704)\n",
    "# items_df.filter(F.col('content_type') == 1).count()\n",
    "\n",
    "# сколько программ в таблице train_df (3704, видимо, только платные и указаны)\n",
    "# train_df.select('item_id').distinct().count()\n",
    "\n",
    "# есть ли в таблице пользователи совсем без покупок (266 out of 1941)\n",
    "# train_df.groupBy('user_id').agg(F.sum('purchase').alias('prch_cnt')).filter(F.col('prch_cnt') == 0).count()\n",
    "\n",
    "# items_df.filter(F.col('channel_id').isNotNull()).count()  # 631864\n",
    "# items_df.filter(F.col('channel_id').isNull()).count()  # 3704\n",
    "\n",
    "# # сколько каналов определяется (207)\n",
    "# items_df.select('channel_id').filter(F.col('channel_id').isNotNull()).distinct().count()\n",
    "\n",
    "# # сколько платных передач, для которых не определен канал (все 3704)\n",
    "# items_df.filter(\n",
    "#     F.col('channel_id').isNull()\n",
    "#     & (F.col('content_type') == 1)\n",
    "# ).select('item_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cddc33e1-c9dc-473a-92b5-527ddce4a6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего покупок: 10904\n",
      "Уникальных пользователей: 1941\n",
      "Уникальных платных программ: 3704\n",
      "Ср. кол-во покупок пользователем: 5.617722823286965\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Расчет основных агрегат по покупкам\n",
    "\n",
    "# Количество всех покупок всеми пользователями\n",
    "ttl_prch_cnt, unq_users_cnt, unq_items_cnt = train_df.agg(\n",
    "    F.sum('purchase'),  # всего покупок\n",
    "    F.countDistinct('user_id'),  # уникальных пользователей\n",
    "    F.countDistinct('item_id'),  # уникальных платных программ\n",
    ").first()\n",
    "\n",
    "user_avg_prch_cnt = ttl_prch_cnt / unq_users_cnt\n",
    "\n",
    "print(\n",
    "    f'Всего покупок: {ttl_prch_cnt}\\n'\n",
    "    f'Уникальных пользователей: {unq_users_cnt}\\n'\n",
    "    f'Уникальных платных программ: {unq_items_cnt}\\n'\n",
    "    f'Ср. кол-во покупок пользователем: {user_avg_prch_cnt}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16cbbedb-29c8-4e14-baee-9fe6d13e7281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==========================================>           (159 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------------+------------------+\n",
      "|item_id|item_prch_cnt|item_prch_prc     |ttl_prhc_prc      |\n",
      "+-------+-------------+------------------+------------------+\n",
      "|67036  |98           |7.153284671532846 |0.8987527512839325|\n",
      "|90762  |83           |6.098457016899339 |0.7611885546588408|\n",
      "|10585  |67           |4.988830975428146 |0.6144534115920763|\n",
      "|8661   |67           |4.82361411087113  |0.6144534115920763|\n",
      "|77442  |64           |4.698972099853157 |0.586940572267058 |\n",
      "|74390  |63           |4.65288035450517  |0.5777696258253852|\n",
      "|9180   |60           |4.4609665427509295|0.5502567865003668|\n",
      "|9919   |59           |4.402985074626866 |0.5410858400586941|\n",
      "|89637  |55           |4.0               |0.5044020542920029|\n",
      "|9911   |55           |4.0087463556851315|0.5044020542920029|\n",
      "+-------+-------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Фичи о программах (айтемах):\n",
    "    + item_id\n",
    "    + количество покупок этого айтема\n",
    "    + количество записей по этому айтему в датафрейме\n",
    "    + доля покупок этого айтема из всех записей по этому айтому\n",
    "    + доля покупок этого айтема из всех покупок\n",
    "\"\"\"\n",
    "\n",
    "### Фичи по покупаемости айтемов\n",
    "items_buying_df = (\n",
    "    train_df\n",
    "    .groupBy('item_id')\n",
    "    .agg(\n",
    "        F.sum('purchase').alias('item_prch_cnt'),  # количество покупок этого айтема\n",
    "        F.count('purchase').alias('item_rows_cnt'),  # количество записей по этому айтему\n",
    "    )\n",
    ")\n",
    "\n",
    "items_buying_df = items_buying_df.withColumn(\n",
    "    'item_prch_prc',  # доля покупок этого айтема из всех записей по этому айтому\n",
    "    F.col('item_prch_cnt') * 100 / F.col('item_rows_cnt')\n",
    ")\n",
    "\n",
    "items_buying_df = items_buying_df.withColumn(\n",
    "    'ttl_prhc_prc',  # доля покупок этого айтема из всех покупок\n",
    "    F.col('item_prch_cnt') * 100 / F.lit(ttl_prch_cnt)\n",
    ")\n",
    "\n",
    "# Удаление технической колонки с количеством строк по этой программе\n",
    "items_buying_df = items_buying_df.drop('item_rows_cnt')\n",
    "\n",
    "items_buying_df = items_buying_df.cache()\n",
    "items_buying_df.orderBy(F.desc('ttl_prhc_prc')).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f661f76-68be-416a-ad9c-0cec98a61dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Проверка\n",
    "# items_buying.orderBy(F.desc('ttl_prhc_prc')).show()\n",
    "# items_buying.agg(F.min('item_rows_cnt'), F.max('item_rows_cnt')).show()\n",
    "# items_buying.filter(F.col('item_prch_cnt') != 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19539ce2-3caa-4d03-aa2e-0a6a698cdad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/22 10:39:06 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 21:===============================================>      (176 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------+----------------------+--------------------+\n",
      "|user_id|user_prch_cnt|user_prch_prc|user_prch_to_avg_ratio|user_prch_percentile|\n",
      "+-------+-------------+-------------+----------------------+--------------------+\n",
      "| 890476|            0|          0.0|                   0.0|                 0.0|\n",
      "| 876234|            0|          0.0|                   0.0|                 0.0|\n",
      "| 797350|            0|          0.0|                   0.0|                 0.0|\n",
      "| 923662|            0|          0.0|                   0.0|                 0.0|\n",
      "| 906790|            0|          0.0|                   0.0|                 0.0|\n",
      "+-------+-------------+-------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Фичи о пользователях:\n",
    "    + общее количество покупок пользователя\n",
    "    + доля платных программ, которые пользовать приобрел\n",
    "    + отношение количества покупок пользователя к среднему количествую покупок\n",
    "    + перцентиль пользователя по количеству покупок\n",
    "\"\"\"\n",
    "\n",
    "### Фичи по покупателской способности пользователей\n",
    "user_prch_df = (\n",
    "    train_df\n",
    "    .groupBy('user_id')\n",
    "    .agg(\n",
    "        F.sum('purchase').alias('user_prch_cnt'),  # количество покупок этого пользователя\n",
    "        F.count('purchase').alias('user_rows_cnt'),  # количество записей по этому пользователю\n",
    "    )\n",
    ")\n",
    "\n",
    "user_prch_df = user_prch_df.withColumn(\n",
    "    'user_prch_prc',  # доля платных программ, которые пользовать приобрел\n",
    "    F.col('user_prch_cnt') * 100 / F.col('user_rows_cnt')\n",
    ")\n",
    "\n",
    "user_prch_df = user_prch_df.withColumn(\n",
    "    'user_prch_to_avg_ratio',  # отношение числа покупок пользователя к среднему числу покупок\n",
    "    F.col('user_prch_cnt') / F.lit(user_avg_prch_cnt)\n",
    ")\n",
    "\n",
    "# Вычисление перцентиля для каждого пользователя\n",
    "user_prch_df = user_prch_df.withColumn(\n",
    "    'user_prch_percentile', \n",
    "    F.percent_rank().over(\n",
    "        Window.orderBy(\"user_prch_cnt\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Удаление технической колонки с количеством строк по этому пользователю\n",
    "user_prch_df = user_prch_df.drop('user_rows_cnt')\n",
    "\n",
    "user_prch_df = user_prch_df.cache()\n",
    "user_prch_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ae1c47e-74b6-41c7-84e8-0437b1214583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Проверка\n",
    "# user_prch_df.agg(F.min('user_rows_cnt'), F.max('user_rows_cnt')).show()\n",
    "# user_prch_df.filter(F.col('user_prch_cnt') > 60).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "038ea9a1-b01a-46ae-b9f0-135a0ee28655",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Исследование данных\n",
    "# items_df.show(1, truncate=False, vertical=True)\n",
    "# items_df.select(F.countDistinct('datetime_availability_start')).show()\n",
    "# items_df.groupBy('datetime_availability_stop').count().show()\n",
    "\n",
    "# для скольких платных программ не указан год (для 4 из 3704)\n",
    "# items_df.groupBy('genres').count().show(100)\n",
    "\n",
    "# # для скольки платных программ указан канал (ни одна запись не содержит инфы о канале)\n",
    "# #   при этом все бесплатные программы содержат инфу о канале\n",
    "# items_df.filter((F.col('content_type') == 1) & F.col('channel_id').isNotNull()).count()\n",
    "\n",
    "# для скольки платных программ указан регион (0)\n",
    "# items_df.filter((F.col('content_type') == 1) & F.col('region_id').isNull()).count()\n",
    "\n",
    "# # сколько уникальных регионов\n",
    "# items_df.groupBy('region_id').count().show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9997edde-173b-42f3-ae02-0c32b94ff66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Формирование бинарных колонок по жанрам\n",
    "\n",
    "# Разделить жанры на отдельные значения в копии датафрейма\n",
    "items_df = items_df.withColumn(\n",
    "    'genres', \n",
    "    F.when(\n",
    "        F.col('genres').isNull(), F.lit([])  # замена пропуска на пустой список\n",
    "    ).otherwise(F.split('genres', ','))\n",
    ")\n",
    "\n",
    "# Список уникальных жанров\n",
    "unq_genres = (\n",
    "    items_df\n",
    "    .filter(F.col('genres').isNotNull())\n",
    "    .select('genres')\n",
    "    .rdd.flatMap(lambda row: row[0])\n",
    "    .distinct()\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Маппиг жанров вида {название_жанра : genre_n, ...}\n",
    "genres_mapping = dict()\n",
    "for unq_gnr in unq_genres:\n",
    "    genres_mapping[unq_gnr] = f'genre_{len(genres_mapping)}'\n",
    "\n",
    "# Создание бинарных колонок для каждого жанра\n",
    "for gnr in genres_mapping:\n",
    "    items_df = items_df.withColumn(\n",
    "        f'{genres_mapping[gnr]}_flg',  # чтобы не использовать кириллицу, заменим название жанра на текст genre_n_flg \n",
    "        F.array_contains(F.col('genres'), gnr).cast(\"integer\")\n",
    "    )\n",
    "\n",
    "# Получение возраста программы\n",
    "items_df = items_df.withColumn(\n",
    "    'item_age',  # возраст программы\n",
    "    2025 - F.col('year').cast(\"integer\")\n",
    ")\n",
    "\n",
    "# Малоинформативные и технические колонки\n",
    "items_cols_to_drop = [\n",
    "    'datetime_availability_start',\n",
    "    'datetime_availability_stop',\n",
    "    'datetime_show_start',\n",
    "    'datetime_show_stop',\n",
    "    'region_id',\n",
    "    'genres',  # данные из колонки уже получены\n",
    "    'year'  # данные из колонки уже получены\n",
    "]\n",
    "\n",
    "# Удаление колонок\n",
    "items_df = items_df.drop(*items_cols_to_drop)\n",
    "\n",
    "items_df = items_df.cache()\n",
    "items_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "29ec0987-7687-4b9a-9b80-2c837f7c2599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TF-IDF на колонке title\n",
    "\n",
    "@F.pandas_udf(ArrayType(StringType()))\n",
    "def get_tokens_list(text_series):\n",
    "    \"\"\"Преобразовать строки в списки токенов.\"\"\"\n",
    "    regex = re.compile(u'[\\w\\d]{2,}', re.U)  # паттерн для поиска слов\n",
    "    tokens_series = text_series.str.lower().apply(lambda row: regex.findall(row) if row else [])\n",
    "    return tokens_series\n",
    "\n",
    "\n",
    "# Токенизация 'title'\n",
    "items_df = items_df.withColumn('title_tokens', get_tokens_list('title'))\n",
    "\n",
    "# Расчет TF с помощью HashingTF\n",
    "hashing_tf = HashingTF(numFeatures=1024, inputCol=\"title_tokens\", outputCol=\"tf_features\")\n",
    "items_df = hashing_tf.transform(items_df)\n",
    "\n",
    "# Расчет IDF с помощью IDF\n",
    "idf = IDF(inputCol='tf_features', outputCol='tfidf_features')\n",
    "items_df = idf.fit(items_df).transform(items_df)\n",
    "\n",
    "items_df = items_df.cache()\n",
    "items_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14bbe802-b322-47b3-8bdb-835da4fb4afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление колонок\n",
    "items_df = items_df.drop('title', 'title_tokens', 'tf_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40d2fecc-c5eb-4bfe-a886-29640fe15c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Поиск оптимального количества кластеров\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# from pyspark.ml.clustering import KMeans\n",
    "# from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# def elbow_method(df, max_clusters=10):\n",
    "#     wssse = []\n",
    "    \n",
    "#     for k in range(2, max_clusters + 1):  # Начинаем с 2 кластеров до максимального значения\n",
    "#         kmeans = KMeans().setK(k).setFeaturesCol('tfidf_features').setPredictionCol('prediction')\n",
    "#         model = kmeans.fit(df)\n",
    "        \n",
    "#         evaluator = ClusteringEvaluator(featuresCol='tfidf_features')\n",
    "#         silhouette = evaluator.evaluate(model.transform(df))\n",
    "        \n",
    "#         cost = model.summary.trainingCost\n",
    "#         wssse.append((k, cost))\n",
    "    \n",
    "#     return wssse\n",
    "\n",
    "# def find_elbow_point(wssse):\n",
    "#     \"\"\"\n",
    "#     Метод для нахождения точки перегиба на основе первой разности.\n",
    "#     \"\"\"\n",
    "#     n_points = len(wssse)\n",
    "#     all_knots = range(len(wssse))\n",
    "#     knots_diff = np.diff(wssse[:, 1]) / np.diff(all_knots)  # Разница между соседними точками\n",
    "#     second_diff = np.diff(knots_diff)  # Вторая разность\n",
    "    \n",
    "#     candidate_idxs = np.where(second_diff > 0)[0]  # Индекс точек, где вторая разность положительна\n",
    "#     if not len(candidate_idxs):\n",
    "#         print(\"Не найдено явной точки перегиба.\")\n",
    "#         return None\n",
    "    \n",
    "#     elbow_point = candidate_idxs[0] + 2  # Выбор первой точки перегиба\n",
    "#     return elbow_point\n",
    "\n",
    "# # DataFrame с колонкой SparseVector\n",
    "# # tfidf_features - название колонки с векторами\n",
    "# df = items_tfidf_df.select('tfidf_features')\n",
    "\n",
    "# # Определяем оптимальное количество кластеров\n",
    "# optimal_clusters = elbow_method(df, max_clusters=10)\n",
    "\n",
    "# # Нахождение точки перегиба\n",
    "# elbow_point = find_elbow_point(np.array(optimal_clusters))\n",
    "# if elbow_point is not None:\n",
    "#     optimal_k = elbow_point\n",
    "#     print(f'Optimal K: {optimal_k}')\n",
    "# else:\n",
    "#     print(\"Точку перегиба не удалось определить автоматически.\")\n",
    "\n",
    "# # optimal_k: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e0ffb63a-9407-47fc-85ef-10befa83ef69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Модель KMeans\n",
    "kmeans = (\n",
    "    KMeans()\n",
    "    .setK(3)\n",
    "    .setSeed(42)\n",
    "    .setFeaturesCol('tfidf_features')\n",
    "    .setPredictionCol('prediction')\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "model = kmeans.fit(items_df)\n",
    "\n",
    "# Прогнозирование и добавление кластеров к данным\n",
    "items_df = model.transform(items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ef2bea82-0abd-4e0e-ba8c-b7def02aee63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_items_df = items_df.drop('tfidf_features').cache()\n",
    "result_items_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c1059516-c553-47da-ac1f-1dedacfcc886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# views_programmes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e579c49a-233e-496d-87a2-1a4056dfd144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+--------+\n",
      "|user_id|item_id|item_type|duration|\n",
      "+-------+-------+---------+--------+\n",
      "|      0|7101053|     live|    1669|\n",
      "|      0|7101054|     live|   39090|\n",
      "|      0|7101054|     live|     841|\n",
      "|      0|6184414|     live|     350|\n",
      "|    257|4436877|     live|    1757|\n",
      "|   1654|7489015|     live|     600|\n",
      "|   1654|7489023|     live|    1500|\n",
      "|   1654|6617053|     live|   14678|\n",
      "|   1654|6438693|     live|     363|\n",
      "|   1654|6526859|     live|     702|\n",
      "+-------+-------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Фичи из истории просмотров\n",
    "\n",
    "# Продолжительность просмотра\n",
    "hist_df = views_programmes_df.withColumn(\n",
    "    'duration',\n",
    "    F.col('ts_end') - F.col('ts_start')\n",
    ")\n",
    "\n",
    "hist_df = hist_df.drop('ts_start', 'ts_end')\n",
    "\n",
    "hist_df = hist_df.cache()\n",
    "hist_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2640f35b-7e31-42b9-9d78-23c4dec2b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+------------+------------------+\n",
      "|user_id|ttl_duration|live_duration|pvr_duration|         pvr_ratio|\n",
      "+-------+------------+-------------+------------+------------------+\n",
      "| 561425|       37699|        30959|        6740|21.770729028715397|\n",
      "+-------+------------+-------------+------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Фичи по времени просмотра:\n",
    "    + общему \n",
    "    + просмотру лайва\n",
    "    + просмотру в записи\n",
    "    + долю в записи от лайва\n",
    "'''\n",
    "\n",
    "hist_df_agg = (\n",
    "    hist_df\n",
    "    .groupBy('user_id')\n",
    "    .agg(\n",
    "        F.sum('duration').alias('ttl_duration'),\n",
    "        F.sum(F.when(F.col('item_type') == 'live', F.col('duration')).otherwise(0)).alias('live_duration'),\n",
    "        F.sum(F.when(F.col('item_type') == 'pvr', F.col('duration')).otherwise(0)).alias('pvr_duration'),\n",
    "    )\n",
    ")\n",
    "\n",
    "hist_df_agg = hist_df_agg.withColumn(\n",
    "    'pvr_ratio',\n",
    "    F.when(F.col('live_duration') != 0, (F.col('pvr_duration') / F.col('live_duration')) * 100).otherwise(0)\n",
    ")\n",
    "\n",
    "hist_df_agg = hist_df_agg.cache()\n",
    "hist_df_agg.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1006512f-1119-4229-a8d2-de9b6bec8099",
   "metadata": {},
   "source": [
    "### Тренировочный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "102c7468-eef5-41e3-8273-e5c17d7b067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    train_df\n",
    "    .join(items_buying_df, how='left', on='item_id')\n",
    "    .join(user_prch_df, how='left', on='user_id')\n",
    "    .join(result_items_df, how='left', on='item_id')\n",
    "    .join(hist_df_agg, how='left', on='user_id')\n",
    ")\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8055eb3c-a386-4a6f-89d4-ed59bb05b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.withColumnRenamed('prediction', 'cluster_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c2bcef6d-5711-4b81-a0d8-524cd1a61016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### Проверка пустых значений\n",
    "# null_counts = dataset.select([F.sum(F.col(c).isNull().cast(\"integer\")).alias(c) for c in dataset.columns])\n",
    "# null_counts.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e3a24d64-437b-4308-8aaf-d432b1cbac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Категориальные колонки\n",
    "feature_columns = [\n",
    "    'item_prch_cnt', 'item_prch_prc', 'ttl_prhc_prc',\n",
    "    'user_prch_cnt', 'user_prch_prc', 'user_prch_to_avg_ratio', 'user_prch_percentile',\n",
    "    'content_type',\n",
    "    'genre_0_flg', 'genre_1_flg', 'genre_2_flg', 'genre_3_flg', 'genre_4_flg', 'genre_5_flg',\n",
    "    'genre_6_flg', 'genre_7_flg', 'genre_8_flg', 'genre_9_flg', 'genre_10_flg', 'genre_11_flg',\n",
    "    'genre_12_flg', 'genre_13_flg', 'genre_14_flg', 'genre_15_flg', 'genre_16_flg', 'genre_17_flg',\n",
    "    'genre_18_flg', 'genre_19_flg', 'genre_20_flg', 'genre_21_flg', 'genre_22_flg', 'genre_23_flg',\n",
    "    'genre_24_flg', 'genre_25_flg', 'genre_26_flg', 'genre_27_flg', 'genre_28_flg', 'genre_29_flg',\n",
    "    'genre_30_flg', 'genre_31_flg', 'genre_32_flg', 'genre_33_flg', 'genre_34_flg', 'genre_35_flg',\n",
    "    'genre_36_flg', 'genre_37_flg', 'genre_38_flg', 'genre_39_flg', 'genre_40_flg', 'genre_41_flg', \n",
    "    'genre_42_flg', 'genre_43_flg', 'genre_44_flg', 'genre_45_flg', 'genre_46_flg', 'genre_47_flg', \n",
    "    'genre_48_flg', 'genre_49_flg', 'genre_50_flg', 'genre_51_flg', 'genre_52_flg', 'genre_53_flg', \n",
    "    'genre_54_flg', 'genre_55_flg', 'genre_56_flg', 'genre_57_flg', 'genre_58_flg', 'genre_59_flg', \n",
    "    'genre_60_flg', 'genre_61_flg', 'genre_62_flg', 'genre_63_flg', 'genre_64_flg', 'genre_65_flg', \n",
    "    'genre_66_flg', 'genre_67_flg', 'genre_68_flg', 'genre_69_flg', 'genre_70_flg', 'genre_71_flg',\n",
    "    'genre_72_flg', 'genre_73_flg', 'genre_74_flg', 'genre_75_flg', 'genre_76_flg', 'genre_77_flg', \n",
    "    'genre_78_flg', 'genre_79_flg', 'genre_80_flg', 'genre_81_flg', 'genre_82_flg', 'genre_83_flg', \n",
    "    'item_age', 'cluster_prediction',\n",
    "    'ttl_duration', 'live_duration', 'pvr_duration', 'pvr_ratio'\n",
    "]\n",
    "\n",
    "# Замена NaN на 0 в тренировочном датасете\n",
    "for col_name in feature_columns:\n",
    "    dataset = dataset.withColumn(\n",
    "        col_name,\n",
    "        F.when(F.isnan(F.col(col_name)), 0).otherwise(F.col(col_name))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49df295b-b88f-42f9-9ac6-e303484bb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.dtypes\n",
    "# dataset.show(1, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435904c1-86bc-4b52-9694-e0e6654f1c30",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d005b789-8e2c-459a-a832-92e0e9799186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# Инициализация классификатора\n",
    "gbt = GBTClassifier(labelCol='purchase', featuresCol=\"features\", maxIter=50, seed=42)\n",
    "\n",
    "# Создаем pipeline (если нужны преобразования)\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "# Делим данные на обучающую и тестовую выборки (рекомендуется)\n",
    "train_df, test_df = dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Обучение модели\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae961a9b-d6f9-4838-9463-7fd028904080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# BACKLOG: сделать сохранение через pickle для удобства (чтобы лежало локально)\n",
    "# Сейчас какая-то ошибка всплывает при загрузке сохраненной модели.\n",
    "\n",
    "# from pyspark.ml.classification import GBTClassificationModel\n",
    "\n",
    "# # Сохранение модели \n",
    "# model_path = \"./models/lab03_gbtc_model\"\n",
    "# model.write().overwrite().save(model_path)\n",
    "\n",
    "# # Загрузка модели\n",
    "# model = GBTClassificationModel.load(\"models/lab03_gbtc_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de1c16-29fb-4f19-b895-48e47f6d170e",
   "metadata": {},
   "source": [
    "### Метрика ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8bc19098-6757-4d36-a501-5a69f6c2df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3488:==============================================>     (178 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC = 0.9376561339039494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Предсказания на тестовой выборке\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Оценка модели\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"purchase\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Test AUC = {auc}\")  \n",
    "\n",
    "# # Пример: показать несколько предсказаний\n",
    "# predictions.select(\"features\", \"purchase\", \"prediction\", \"probability\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f7c69-ecd1-4957-a0ca-0902a6fe83f8",
   "metadata": {},
   "source": [
    "### Инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a7f211de-8c38-4c93-b800-5daf594f780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_dataset = (\n",
    "    infer_df\n",
    "    .join(items_buying_df, how='left', on='item_id')\n",
    "    .join(user_prch_df, how='left', on='user_id')\n",
    "    .join(result_items_df, how='left', on='item_id')\n",
    "    .join(hist_df_agg, how='left', on='user_id')\n",
    ")\n",
    "\n",
    "inf_dataset = inf_dataset.withColumnRenamed('prediction', 'cluster_prediction')\n",
    "\n",
    "inf_dataset = inf_dataset.cache()\n",
    "inf_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "762ef8dd-cb1a-44d7-ae89-6c4434102d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Проверка соответсвия обучающего и инференс датасетов\n",
    "# assert inf_dataset.columns == dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dbbd5044-3d0c-4a08-b94f-ccb8ff0d8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Проверка пустых значений\n",
    "# null_counts = inf_dataset.select([F.sum(F.col(c).isNull().cast(\"integer\")).alias(c) for c in inf_dataset.columns])\n",
    "# null_counts.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5717e386-ec34-43eb-906e-ff8a007be99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Замена NaN на 0\n",
    "for col_name in feature_columns:\n",
    "    inf_dataset = inf_dataset.withColumn(\n",
    "        col_name,\n",
    "        F.when(F.col(col_name).isNull(), 0).otherwise(F.col(col_name))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8b268a43-3da5-427e-b712-29e715ec71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказания на тестовой выборке\n",
    "inf_predicts = model.transform(inf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b23db4f5-09ef-49c2-a2e7-f48fdb6e0d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_predicts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "06499ba4-d318-4deb-b2f8-c9106ea8b844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/22 20:39:43 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1230.3 KiB\n",
      "[Stage 3403:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+\n",
      "|user_id|item_id|            purchase|\n",
      "+-------+-------+--------------------+\n",
      "| 894161|   8389|0.022937152534723282|\n",
      "| 894261|   8389| 0.02204175293445587|\n",
      "| 894307|   8389| 0.02324526011943817|\n",
      "+-------+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Получение итогового датасета с вероятностями покупки\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, FloatType, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# UDF для преобразования DenseVector в массив float\n",
    "vector_to_array_udf = F.udf(lambda v: v.toArray().tolist() if v is not None else None, ArrayType(FloatType()))\n",
    "\n",
    "# Преобразуем колонку probability в массивы\n",
    "inf_res = inf_predicts.withColumn('probability_array', vector_to_array_udf('probability'))\n",
    "\n",
    "# pandas_udf для извлечения второго элемента из массива\n",
    "@F.pandas_udf(DoubleType())\n",
    "def extract_second_element(vec_series: pd.Series) -> pd.Series:\n",
    "    return vec_series.apply(lambda arr: float(arr[1]) if arr is not None and len(arr) > 1 else None)\n",
    "\n",
    "# Применяем pandas_udf к новой колонке\n",
    "inf_res = inf_res.withColumn('second_value', extract_second_element('probability_array'))\n",
    "\n",
    "inf_res = inf_res.select(\n",
    "    'user_id', \n",
    "    'item_id',\n",
    "    F.col('second_value').alias('purchase')\n",
    ")\n",
    "\n",
    "inf_res = inf_res.cache()\n",
    "inf_res.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9398fb8-1f85-4bac-8397-3ba2909a128b",
   "metadata": {},
   "source": [
    "### Сохранение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "837ec6d1-eb2b-4724-82a7-3688dff403b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_res_df = (\n",
    "#     inf_res\n",
    "#     .orderBy(\n",
    "#         F.asc('user_id'),\n",
    "#         F.asc('item_id')\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "62522143-8cd8-43cd-9bab-9cd87923174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_res_df.toPandas().to_csv(\"lab03.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5940f-2e13-4c47-8949-b43fd5ef11cd",
   "metadata": {},
   "source": [
    "### Завершение контекста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f645d962-00bf-4d07-97b2-91c40143d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
